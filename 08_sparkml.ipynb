{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](./docs/images/itam_logo.png)\n",
    "\n",
    "M. Sc. Liliana Millán Núñez liliana.millan@itam.mx\n",
    "\n",
    "Marzo 2020\n",
    "\n",
    "## SparkML\n",
    "\n",
    "### Agenda \n",
    "\n",
    "+ SparkML\n",
    "    + Pipelines\n",
    "    + Feature engineering\n",
    "    + Clasificación y regresión\n",
    "    + Agrupación\n",
    "    + Tuneo de hiperparámetros\n",
    "+ Ejemplo\n",
    "\n",
    "### Spark ML\n",
    "\n",
    "`spark.ml` es el módulo de *machine learning* de Spark, diseñado para realizar *machine learning* dentro de spark de manera escalable, sencilla y aprovechando el procesamiento en paralelo.\n",
    "\n",
    "**Características** \n",
    "\n",
    "+ Tiene algoritmos de ML ya implementados con la modificaciones necesarias para aprovechar el ambiente distribuido donde vive Spark: clasificación, regresión, agrupación, filtros colaborativos, etc. \n",
    "+ Tiene implementaciones de funciones que ocupamos para hacer *feature engineering*: *feature extraction*, *feature selection*, transformaciones, reducción de dimensionalidad.\n",
    "+ Permite generar *pipelines* en Spark al estilo de los pipelines de `scikitlearn`.\n",
    "+ Tiene una libería de utilerías con álgebra lineal, estadística, manejo de datos, etc.\n",
    "\n",
    "¿Por qué hay un `spark.mllib` y un `spark.ml`? \n",
    "\n",
    "En la primer versión de Spark no existía la abstracción de *DataFrame* -el *wrapper* de los RDD- y todos los algoritmos de ML desarrollados en Spark interactuaban directamente con el RDD, todas estas implementaciones se encuentran en el paquete `spark.mllib` -que ya está descontinuada-. Una vez que salió la versión 2 de Spark y con ella los nuevos objetos *SparkSession* y *DataFrame* los algoritmos de ML fueron modificados -algunos- para que solo tengan interacción con la abstracción *DataFrame* y con ello surgió la librería `spark.ml` que es la que utilizaremos nosotros. Aún no están todos los algoritmos de `spark.mllib` implementados en *DataFrame*, en la versión 2.4.5 de Spark, la librería de `spark.mllib` ya está en estatus de solo mantenimiento para que a partir de Spark 3.0 la librería será removida completamente de Spark y solo ocupar la interacción con los *DataFrames*. \n",
    "\n",
    "*Anyway* Para confundir más a la banda, el nombre oficial de la herramienta que ocupa Spark para ML se conoce como **MLlib** (╯°□°)╯︵ ┻━┻ aunque realmente se refieren a la librería `spark.ml`.\n",
    "\n",
    "[Spark ML API](https://spark.apache.org/docs/latest/ml-guide.html)\n",
    "\n",
    "#### Pipelines \n",
    "\n",
    "El diseño de los *pipelines* de Spark está inspirado en los *pipelines* de `scikit-learn`. Un *pipeline* en Spark está formado por los siguientes elementos: \n",
    "\n",
    "+ **DataFrame:** API que ocupa los *DataFrame* de SparkSQL para poder agregar otros tipos de datos que pueden ser útiles para ML -*vector*-\n",
    "\n",
    "+ **Transformer:** Algoritmo que transforma un *DataFrame* en otro DataFrame, recuerda que los *DataFrame* en Spark envuelven a un RDD y un RDD no puede ser modificado!. Para hacer una transformación se ocupa el método `transform()`. Los casos en los que ocuparemos un `transform` pueden ser agregar una nueva columna -por ejemplo *feature engineering*-, o por ejemplo una vez que se ha pasado un modelo de aprendizaje poner la respuesta final del modelo como parte del *DataFrame* original -etiqueta, score-. \n",
    "\n",
    "+ **Estimator:** Algoritmos que se aplican a un *DataFrame* para producir un *Transformer*. Los estimators son los que ocupan el método `fit()` para poder realizar un entrenamiento. El método `fit` recibe como parámetro un *DataFrame* y devuelve un modelo -que es un *transformer*-. Por ejemplo: Un algoritmo de regresión lineal es un *estimator* que tiene su método `fit` a través del cual entrena el algoritmo. \n",
    "\n",
    "\n",
    "$\\rightarrow$ Es importante conocer que por cada instancia de un *transformer* o *estimator* se genera un ID a través del cuál es reconocido durante todo el *pipeline* y por lo tanto podemos llamarlo más adelante en el pipeline.\n",
    "\n",
    "+ **Pipeline:** Es una secuencia de procesos/etapas generado por *transformers* y *estimators* para hacer un *workflow* de ML. Cada *transformer*/*estimator* es una etapa dentro de la secuencia del *pipeline*, cada paso se corre en el orden establecido y el *DataFrame* de entrada es transformado por cada paso, si el paso es un *transformer* entonces se le aplica el método `transform` y si el paso es un `estimator` se le aplica el método `fit`. \n",
    "\n",
    "\n",
    "![](./docs/images/spark_estimator_transformer.png)\n",
    "<br>\n",
    "\n",
    "Por ejemplo: Si tuviéramos un texto al cuál quisieramos aplicarle un análisis de sentimiento, el *pipeline* podría consistir en los siguientes pasos: \n",
    "\n",
    "Suposiciones: \n",
    "+ Tenemos un corpus.\n",
    "+ Tenemos las palabras asociadas a un sentimiento.\n",
    "\n",
    "+ Separar cada documento en palabras.\n",
    "+ Convertir cada palabra de cada documento en un vector numérico.\n",
    "+ Utilizando el vector numérico y las etiquetas asociadas -del sentimiento- ocupar un modelo de clasificación \n",
    "\n",
    "![](./docs/images/spark_pipelines.png)\n",
    "\n",
    "<br>\n",
    "\\* Fuente: [Spark ML Guide](https://spark.apache.org/docs/latest/ml-guide.html)\n",
    "\n",
    "\n",
    "![](./docs/images/pointer.png) En Spark, un pipeline **es** un *estimator* (al igual que en `sklearn`, por lo que puede hacer llamada al método `fit`, al hacer esto se genera un *PipelineModel* -que es un *transformer*-. Cuando querramos ocupar modelos entrenados para producción deberemos ocupar el *PipelineModel* generado en el momento de entranamiento al hacer una llamada a su método *transform*, de esta manera todos los *estimators* del *pipeline* original son convertidos a *transformer* asegurándonos de que en pruebas tendremos los mismos pasos/trasnformaciones ocupados para el entrenamiento del modelo. ╭(◔ ◡ ◔)/\n",
    "\n",
    "![](./docs/images/spark_pipeline_model.png)\n",
    "<br>\n",
    "\\*Fuente: [Spark ML Guide](https://spark.apache.org/docs/latest/api/python/pyspark.ml.html#module-pyspark.ml) \n",
    "\n",
    "Un *Pipeline* en Spark está representado como un DAG, el ejemplo anterior es un DAG lineal, pero no necesariamente deben ser lineales, basta con que cumplan las características de ser un DAG -grafo **acíclico** dirigido-. Es por esta razón que cada instanciación de un *transformer* o *estimator* tiene asociado un ID y debe ser único, si necesitaramos un mismo *transformer* en el *pipeline* requerimos de generar otro *transformer* -aunque tenga el mismo código- :( (ya sé! esto medio que le da en la ma al principio de *reuse* pero ... por el momento así se resuelve en Spark en pro de tener un *pipeline*), Spark revisa en tiempo de ejecución que no se rompa \"algo\" antes de correr el *pipeline* -*lazy*-\n",
    "\n",
    "+ **Parameter:** API con la que se pueden compartir parámetros entre *Estimators* y *Transformers*. Ocupamos el objeto `Param` que es un parámetro nombrado con documentación auto contenida en un `ParamMap` -diccionario de parámetro, valor-.\n",
    "\n",
    "En Spark hay dos maneras de pasar parámetros a los algoritmos de ML:\n",
    "\n",
    "1. Configurar los parámetros fijos de los algoritmos a ocupar (*setters*)\n",
    "2. Pasar un `ParamMap` con los parámetros y sus valores a través de `fit` o `transform`, si se envían parámetros de esta manera se hace *override* a los específicados vía *setters*\n",
    "\n",
    "Lo lindo de estos objetos es que cada definición dentro del `ParamMap` es \"atado\" a un *estimator* o *transformer* en específico -a través del ID antes mencionado-. Por ejemplo: si tuvieramos en un *pipeline* dos regresiones logísticas -`lr1` y `lr2`- podríamos ocupar un `ParamMap` que establezca el valor de las iteraciones máximas de cada regresión: \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pyspark'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-5ddfd91fd7b3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_line_magic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'matplotlib'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'inline'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mpyspark\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mpyspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msql\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msession\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSparkSession\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mpyspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mml\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinalg\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mpyspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msql\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunctions\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'pyspark'"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import pyspark \n",
    "from pyspark.sql.session import SparkSession\n",
    "from pyspark.ml.linalg import *\n",
    "from pyspark.sql.functions import *  \n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "\n",
    "import time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import findspark\n",
    "findspark.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "itam",
   "language": "python",
   "name": "itam"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
