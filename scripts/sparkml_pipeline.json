{"paragraphs":[{"user":"anonymous","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"python","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"editorMode":"ace/mode/python"},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1585691388801_1804167925","id":"20200331-214948_1505795863","dateCreated":"2020-03-31T21:49:48+0000","status":"FINISHED","progressUpdateIntervalMs":500,"focus":true,"$$hashKey":"object:242","text":"%pyspark\nspark","dateUpdated":"2020-03-31T21:49:56+0000","dateFinished":"2020-03-31T21:50:22+0000","dateStarted":"2020-03-31T21:49:56+0000","results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"<pyspark.sql.session.SparkSession object at 0x7f6a97febf10>\n"}]}},{"text":"%pyspark\nfrom pyspark.ml import Pipeline\nfrom pyspark.ml.classification import LogisticRegression\nfrom pyspark.ml.feature import HashingTF, Tokenizer","user":"anonymous","dateUpdated":"2020-03-31T21:51:59+0000","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"python","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"editorMode":"ace/mode/python"},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1585691506843_1181568728","id":"20200331-215146_2112040330","dateCreated":"2020-03-31T21:51:46+0000","status":"FINISHED","progressUpdateIntervalMs":500,"focus":true,"$$hashKey":"object:567","dateFinished":"2020-03-31T21:51:59+0000","dateStarted":"2020-03-31T21:51:59+0000","results":{"code":"SUCCESS","msg":[]}},{"text":"%md\n\nAl igual que en el ejemplo anterior, creamos un *DataFrame* a través de una lista con los datos de entrenamiento, la lista esta formada por tuplas (id, texto, label). Esta forma no nos servirá para poder meterla en los objetos de ML, pero más adelante arreglaremos esto.\n","user":"anonymous","dateUpdated":"2020-03-31T22:02:19+0000","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"editorMode":"ace/mode/markdown","editorHide":true,"tableHide":false},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1585691526704_-127185150","id":"20200331-215206_552492607","dateCreated":"2020-03-31T21:52:06+0000","status":"FINISHED","progressUpdateIntervalMs":500,"focus":true,"$$hashKey":"object:642","dateFinished":"2020-03-31T22:02:19+0000","dateStarted":"2020-03-31T22:02:19+0000","results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<p>Al igual que en el ejemplo anterior, creamos un <em>DataFrame</em> a través de una lista con los datos de entrenamiento, la lista esta formada por tuplas (id, texto, label). Esta forma no nos servirá para poder meterla en los objetos de ML, pero más adelante arreglaremos esto.</p>\n</div>"}]}},{"text":"%pyspark\n\ntraining = spark.createDataFrame([\n    (0, \"a b c d e spark\", 1.0),\n    (1, \"b d\", 0.0),\n    (2, \"spark f g h\", 1.0),\n    (3, \"hadoop mapreduce\", 0.0)\n], [\"id\", \"text\", \"label\"])\n\n","user":"anonymous","dateUpdated":"2020-03-31T21:52:42+0000","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"python","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"editorMode":"ace/mode/python"},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1585691553234_449086009","id":"20200331-215233_883240924","dateCreated":"2020-03-31T21:52:33+0000","status":"FINISHED","progressUpdateIntervalMs":500,"focus":true,"$$hashKey":"object:750","dateFinished":"2020-03-31T21:52:43+0000","dateStarted":"2020-03-31T21:52:42+0000","results":{"code":"SUCCESS","msg":[]}},{"text":"%md\n\nDefinimos los *transformers*: `Tokenizer` y `HashingTF`, y los  *estimators*: `LogisticRegression` que ocuparemos. Nota que aquí no hemos hecho ningún `fit` todavia... la magia vendrá más adelante ;) `Tokenizer` convierte el string de entrada (inputCol) a minúsculas y separa en palabras utilizando como separador el espacio\n","user":"anonymous","dateUpdated":"2020-03-31T22:03:14+0000","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"editorMode":"ace/mode/markdown","editorHide":true,"tableHide":false},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1585691577829_-304383987","id":"20200331-215257_186086078","dateCreated":"2020-03-31T21:52:57+0000","status":"FINISHED","progressUpdateIntervalMs":500,"focus":true,"$$hashKey":"object:822","dateFinished":"2020-03-31T22:03:14+0000","dateStarted":"2020-03-31T22:03:14+0000","results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<p>Definimos los <em>transformers</em>: <code>Tokenizer</code> y <code>HashingTF</code>, y los <em>estimators</em>: <code>LogisticRegression</code> que ocuparemos. Nota que aquí no hemos hecho ningún <code>fit</code> todavia&hellip; la magia vendrá más adelante ;) <code>Tokenizer</code> convierte el string de entrada (inputCol) a minúsculas y separa en palabras utilizando como separador el espacio</p>\n</div>"}]}},{"text":"%pyspark\n\ntokenizer = Tokenizer(inputCol=\"text\", outputCol=\"words\")\n","user":"anonymous","dateUpdated":"2020-03-31T21:53:42+0000","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"python","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"editorMode":"ace/mode/python"},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1585691611682_-1705721524","id":"20200331-215331_28085663","dateCreated":"2020-03-31T21:53:31+0000","status":"FINISHED","progressUpdateIntervalMs":500,"focus":true,"$$hashKey":"object:912","dateFinished":"2020-03-31T21:53:42+0000","dateStarted":"2020-03-31T21:53:42+0000","results":{"code":"SUCCESS","msg":[]}},{"text":"%md\n\n`HashingTF` permite *hashear* cada palabra utilizando `MurmurHash3` convirtiendo el `hash` generado en el índice a poner en el \"TDM\". Este método optimiza el tiempo para generar el TDM de TF-IDF \"normal\". Para evitar colisiones en la conversión a hash se aumenta el número de buckets -se recomienda ocupar potencias de 2 para balancear las cubetas-. \nNota que en este *transformer* estamos ocupando como entrada la salida del *transformer*  `Tokenizer`\n","user":"anonymous","dateUpdated":"2020-03-31T22:04:07+0000","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"editorMode":"ace/mode/markdown","editorHide":true,"tableHide":false},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1585691629641_990897479","id":"20200331-215349_1455023872","dateCreated":"2020-03-31T21:53:49+0000","status":"FINISHED","progressUpdateIntervalMs":500,"focus":true,"$$hashKey":"object:984","dateFinished":"2020-03-31T22:04:07+0000","dateStarted":"2020-03-31T22:04:07+0000","results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<p><code>HashingTF</code> permite <em>hashear</em> cada palabra utilizando <code>MurmurHash3</code> convirtiendo el <code>hash</code> generado en el índice a poner en el &ldquo;TDM&rdquo;. Este método optimiza el tiempo para generar el TDM de TF-IDF &ldquo;normal&rdquo;. Para evitar colisiones en la conversión a hash se aumenta el número de buckets -se recomienda ocupar potencias de 2 para balancear las cubetas-.<br/>Nota que en este <em>transformer</em> estamos ocupando como entrada la salida del <em>transformer</em> <code>Tokenizer</code></p>\n</div>"}]}},{"text":"%pyspark\n\nhashingTF = HashingTF(inputCol=tokenizer.getOutputCol(), outputCol=\"features\")\n","user":"anonymous","dateUpdated":"2020-03-31T21:54:33+0000","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"python","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"editorMode":"ace/mode/python"},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1585691661761_-1021773653","id":"20200331-215421_714392098","dateCreated":"2020-03-31T21:54:21+0000","status":"FINISHED","progressUpdateIntervalMs":500,"focus":true,"$$hashKey":"object:1074","dateFinished":"2020-03-31T21:54:33+0000","dateStarted":"2020-03-31T21:54:33+0000","results":{"code":"SUCCESS","msg":[]}},{"text":"%md\n\nOcuparemos una regresión logística de nuex","user":"anonymous","dateUpdated":"2020-03-31T22:04:17+0000","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"editorMode":"ace/mode/markdown","editorHide":true,"tableHide":false},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1585691683025_659978492","id":"20200331-215443_1028764415","dateCreated":"2020-03-31T21:54:43+0000","status":"FINISHED","progressUpdateIntervalMs":500,"focus":true,"$$hashKey":"object:1146","dateFinished":"2020-03-31T22:04:17+0000","dateStarted":"2020-03-31T22:04:17+0000","results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<p>Ocuparemos una regresión logística de nuex</p>\n</div>"}]}},{"text":"%pyspark\nlr = LogisticRegression(maxIter=10, regParam=0.001)","user":"anonymous","dateUpdated":"2020-03-31T21:55:19+0000","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"python","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"editorMode":"ace/mode/python"},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1585691707993_-1909026161","id":"20200331-215507_631602473","dateCreated":"2020-03-31T21:55:07+0000","status":"FINISHED","progressUpdateIntervalMs":500,"focus":true,"$$hashKey":"object:1236","dateFinished":"2020-03-31T21:55:19+0000","dateStarted":"2020-03-31T21:55:19+0000","results":{"code":"SUCCESS","msg":[]}},{"text":"%md\n\nAqui viene lo bonito... definimos un *Pipeline* que tiene como etapas/pasos primero el *tokenizer*, luego el *hashing* y luego la regresión logística. Aquí estamos definiendo el flujo de procesamiento, el DAG! ","user":"anonymous","dateUpdated":"2020-03-31T22:04:55+0000","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"editorMode":"ace/mode/markdown","editorHide":true,"tableHide":false},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1585691726477_-519441784","id":"20200331-215526_243434574","dateCreated":"2020-03-31T21:55:26+0000","status":"FINISHED","progressUpdateIntervalMs":500,"focus":true,"$$hashKey":"object:1320","dateFinished":"2020-03-31T22:04:55+0000","dateStarted":"2020-03-31T22:04:55+0000","results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<p>Aqui viene lo bonito&hellip; definimos un <em>Pipeline</em> que tiene como etapas/pasos primero el <em>tokenizer</em>, luego el <em>hashing</em> y luego la regresión logística. Aquí estamos definiendo el flujo de procesamiento, el DAG!</p>\n</div>"}]}},{"text":"%pyspark\n\npipeline = Pipeline(stages=[tokenizer, hashingTF, lr])\n","user":"anonymous","dateUpdated":"2020-03-31T21:55:58+0000","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"python","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"editorMode":"ace/mode/python"},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1585691746825_-1578462313","id":"20200331-215546_82010273","dateCreated":"2020-03-31T21:55:46+0000","status":"FINISHED","progressUpdateIntervalMs":500,"focus":true,"$$hashKey":"object:1392","dateFinished":"2020-03-31T21:55:58+0000","dateStarted":"2020-03-31T21:55:58+0000","results":{"code":"SUCCESS","msg":[]}},{"text":"%md\n\nVoila, solo se requiere de hacer `fit` al *pipeline* para que esto funcione como un *pipeline*, siguiendo el orden de los pasos establecidos en la  definicion del *pipeline* :) ... recuerda que el `fit` hace el entrenamiento una vez que ya definimos las configuraciones de los objetos que ocuparemos (*transformers* y *estimators*)","user":"anonymous","dateUpdated":"2020-03-31T22:05:59+0000","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"editorMode":"ace/mode/markdown","editorHide":true,"tableHide":false},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1585691766873_-1192091355","id":"20200331-215606_1746353019","dateCreated":"2020-03-31T21:56:06+0000","status":"FINISHED","progressUpdateIntervalMs":500,"focus":true,"$$hashKey":"object:1482","dateFinished":"2020-03-31T22:05:59+0000","dateStarted":"2020-03-31T22:05:59+0000","results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<p>Voila, solo se requiere de hacer <code>fit</code> al <em>pipeline</em> para que esto funcione como un <em>pipeline</em>, siguiendo el orden de los pasos establecidos en la definicion del <em>pipeline</em> :) &hellip; recuerda que el <code>fit</code> hace el entrenamiento una vez que ya definimos las configuraciones de los objetos que ocuparemos (*transformers* y <em>estimators</em>)</p>\n</div>"}]}},{"text":"%pyspark\n\nmodel = pipeline.fit(training)","user":"anonymous","dateUpdated":"2020-03-31T21:57:17+0000","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"python","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"editorMode":"ace/mode/python"},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1585691811047_-505918733","id":"20200331-215651_655590529","dateCreated":"2020-03-31T21:56:51+0000","status":"FINISHED","progressUpdateIntervalMs":500,"focus":true,"$$hashKey":"object:1588","dateFinished":"2020-03-31T21:57:27+0000","dateStarted":"2020-03-31T21:57:17+0000","results":{"code":"SUCCESS","msg":[]}},{"text":"%md\nCreamos el *dataframe* de pruebas *mock*! -> Nota que aqui no hay *label*!!!! (asi funcionaría en producción cierto!)","user":"anonymous","dateUpdated":"2020-03-31T22:06:26+0000","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"editorMode":"ace/mode/markdown","editorHide":true,"tableHide":false},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1585691824935_-1342519506","id":"20200331-215704_898699716","dateCreated":"2020-03-31T21:57:04+0000","status":"FINISHED","progressUpdateIntervalMs":500,"focus":true,"$$hashKey":"object:1672","dateFinished":"2020-03-31T22:06:26+0000","dateStarted":"2020-03-31T22:06:26+0000","results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<p>Creamos el <em>dataframe</em> de pruebas <em>mock</em>! -&gt; Nota que aqui no hay <em>label</em>!!!! (asi funcionaría en producción cierto!)</p>\n</div>"}]}},{"text":"%pyspark\n\ntest = spark.createDataFrame([\n    (4, \"spark i j k\"),\n    (5, \"l m n\"),\n    (6, \"spark hadoop spark\"),\n    (7, \"apache hadoop\")\n], [\"id\", \"text\"])\n\n","user":"anonymous","dateUpdated":"2020-03-31T21:57:36+0000","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"python","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"editorMode":"ace/mode/python"},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1585691851804_-1397547515","id":"20200331-215731_1175414367","dateCreated":"2020-03-31T21:57:31+0000","status":"FINISHED","progressUpdateIntervalMs":500,"focus":true,"$$hashKey":"object:1775","dateFinished":"2020-03-31T21:57:36+0000","dateStarted":"2020-03-31T21:57:36+0000","results":{"code":"SUCCESS","msg":[]}},{"text":"%md\n\nLixto, \"ejecutamos\" el *pipeline* haciendo un `transform` al *pipeline* para obtener las predicciones del *set* de pruebas","user":"anonymous","dateUpdated":"2020-03-31T22:07:26+0000","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"editorMode":"ace/mode/markdown","editorHide":true,"tableHide":false},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1585691864364_777354961","id":"20200331-215744_2033443550","dateCreated":"2020-03-31T21:57:44+0000","status":"FINISHED","progressUpdateIntervalMs":500,"focus":true,"$$hashKey":"object:1847","dateFinished":"2020-03-31T22:07:26+0000","dateStarted":"2020-03-31T22:07:26+0000","results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<p>Lixto, &ldquo;ejecutamos&rdquo; el <em>pipeline</em> haciendo un <code>transform</code> al <em>pipeline</em> para obtener las predicciones del <em>set</em> de pruebas</p>\n</div>"}]}},{"text":"%pyspark\n\nprediction = model.transform(test)","user":"anonymous","dateUpdated":"2020-03-31T21:58:20+0000","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"python","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"editorMode":"ace/mode/python"},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1585691881611_1536015809","id":"20200331-215801_528033461","dateCreated":"2020-03-31T21:58:01+0000","status":"FINISHED","progressUpdateIntervalMs":500,"focus":true,"$$hashKey":"object:1948","dateFinished":"2020-03-31T21:58:20+0000","dateStarted":"2020-03-31T21:58:20+0000","results":{"code":"SUCCESS","msg":[]}},{"text":"%md\n\nDe nuevo, `prediction` es un DataFrame generado con un *transformer* generado a su vez de estimadores y transformadores :) \n\nSeleccionamos las columnas que queremos ver ","user":"anonymous","dateUpdated":"2020-03-31T21:59:27+0000","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"editorMode":"ace/mode/markdown","editorHide":true,"tableHide":false},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1585691897078_491948897","id":"20200331-215817_1827672149","dateCreated":"2020-03-31T21:58:17+0000","status":"FINISHED","progressUpdateIntervalMs":500,"focus":true,"$$hashKey":"object:2020","dateFinished":"2020-03-31T21:59:27+0000","dateStarted":"2020-03-31T21:59:27+0000","results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<p>De nuevo, <code>prediction</code> es un DataFrame generado con un <em>transformer</em> generado a su vez de estimadores y transformadores :) </p>\n<p>Seleccionamos las columnas que queremos ver</p>\n</div>"}]}},{"text":"%pyspark\n\nselected = prediction.select(\"id\", \"text\", \"probability\", \"prediction\")\nfor row in selected.collect():\n    rid, text, prob, prediction = row\n    print(\"({}, {}) --> prob={}, prediction={}\".format( \\\n    rid, text, str(prob), prediction))","user":"anonymous","dateUpdated":"2020-03-31T21:59:32+0000","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{}},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1585691396053_1431969137","id":"20200331-214956_503898467","dateCreated":"2020-03-31T21:49:56+0000","status":"FINISHED","progressUpdateIntervalMs":500,"focus":true,"$$hashKey":"object:319","dateFinished":"2020-03-31T21:59:32+0000","dateStarted":"2020-03-31T21:59:32+0000","results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"(4, spark i j k) --> prob=[0.15964077387874612,0.8403592261212539], prediction=1.0\n(5, l m n) --> prob=[0.8378325685476687,0.16216743145233126], prediction=0.0\n(6, spark hadoop spark) --> prob=[0.0692663313297633,0.9307336686702367], prediction=1.0\n(7, apache hadoop) --> prob=[0.9821575333444216,0.01784246665557836], prediction=0.0\n"}]}},{"text":"%pyspark\n","user":"anonymous","dateUpdated":"2020-03-31T21:50:48+0000","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1585691448424_1769428974","id":"20200331-215048_1025466494","dateCreated":"2020-03-31T21:50:48+0000","status":"READY","progressUpdateIntervalMs":500,"focus":true,"$$hashKey":"object:426"}],"name":"sparkml_ex2","id":"2F4NUMZM7","noteParams":{},"noteForms":{},"angularObjects":{},"config":{"isZeppelinNotebookCronEnable":false,"looknfeel":"default","personalizedMode":"false"},"info":{}}