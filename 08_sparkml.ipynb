{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](./images/itam_logo.png)\n",
    "\n",
    "M. Sc. Liliana Millán Núñez liliana.millan@itam.mx\n",
    "\n",
    "Marzo 2021\n",
    "\n",
    "\n",
    "## SparkML\n",
    "\n",
    "### Agenda \n",
    "\n",
    "+ SparkML\n",
    "    + Pipelines\n",
    "    + Feature engineering\n",
    "    + Clasificación y regresión\n",
    "    + Agrupación\n",
    "    + Tuneo de hiperparámetros\n",
    "+ Ejemplo\n",
    "\n",
    "### Spark ML\n",
    "\n",
    "`spark.ml` es el módulo de *machine learning* de Spark, diseñado para realizar *machine learning* dentro de spark de manera escalable, sencilla y aprovechando el procesamiento en paralelo.\n",
    "\n",
    "**Características** \n",
    "\n",
    "+ Tiene algoritmos de ML ya implementados con la modificaciones necesarias para aprovechar el ambiente distribuido donde vive Spark: clasificación, regresión, agrupación, filtros colaborativos, etc. \n",
    "+ Tiene implementaciones de funciones que ocupamos para hacer *feature engineering*: *feature extraction*, *feature selection*, transformaciones, reducción de dimensionalidad.\n",
    "+ Permite generar *pipelines* en Spark al estilo de los pipelines de `scikitlearn`.\n",
    "+ Tiene una libería de utilerías con álgebra lineal, estadística, manejo de datos, etc.\n",
    "\n",
    "¿Por qué hay un `spark.mllib` y un `spark.ml`? \n",
    "\n",
    "En la primer versión de Spark no existía la abstracción de *DataFrame* -el *wrapper* de los RDD- y todos los algoritmos de ML desarrollados en Spark interactuaban directamente con el RDD, todas estas implementaciones se encuentran en el paquete `spark.mllib` -que ya está descontinuada-. Una vez que salió la versión 2 de Spark y con ella los nuevos objetos *SparkSession* y *DataFrame* los algoritmos de ML fueron modificados -algunos- para que solo tengan interacción con la abstracción *DataFrame* y con ello surgió la librería `spark.ml` que es la que utilizaremos nosotros. Aún no están todos los algoritmos de `spark.mllib` implementados en *DataFrame*, en la versión 2.4.5 de Spark, la librería de `spark.mllib` ya está en estatus de solo mantenimiento para que a partir de Spark 3.0 la librería será removida completamente de Spark y solo ocupar la interacción con los *DataFrames*. \n",
    "\n",
    "*Anyway* Para confundir más a la banda, el nombre oficial de la herramienta que ocupa Spark para ML se conoce como **MLlib** (╯°□°)╯︵ ┻━┻ aunque realmente se refieren a la librería `spark.ml`.\n",
    "\n",
    "[Spark ML API](https://spark.apache.org/docs/2.4.7/ml-guide.html)\n",
    "\n",
    "#### Pipelines \n",
    "\n",
    "El diseño de los *pipelines* de Spark está inspirado en los *pipelines* de `scikit-learn`. Un *pipeline* en Spark está formado por los siguientes elementos: \n",
    "\n",
    "+ **DataFrame:** API que ocupa los *DataFrame* de SparkSQL para poder agregar otros tipos de datos que pueden ser útiles para ML -*vector*-\n",
    "\n",
    "+ **Transformer:** Algoritmo que transforma un *DataFrame* en otro DataFrame, recuerda que los *DataFrame* en Spark envuelven a un RDD y un RDD no puede ser modificado!. Para hacer una transformación se ocupa el método `transform()`. Los casos en los que ocuparemos un `transform` pueden ser agregar una nueva columna -por ejemplo *feature engineering*-, o por ejemplo una vez que se ha pasado un modelo de aprendizaje poner la respuesta final del modelo como parte del *DataFrame* original -etiqueta, score-. \n",
    "\n",
    "+ **Estimator:** Procesos o algoritmos que se aplican a un *DataFrame* para producir un *Transformer*. Los estimadores son los que ocupan el método `fit()` para poder realizar un entrenamiento. El método `fit` recibe como parámetro un *DataFrame* y devuelve un modelo -que es un *transformer*-. Por ejemplo: Un algoritmo de regresión lineal es un *estimator* que tiene su método `fit` a través del cual entrena el algoritmo. \n",
    "\n",
    "\n",
    "$\\rightarrow$ Es importante conocer que por cada instancia de un *transformer* o *estimator* se genera un ID a través del cuál es reconocido durante todo el *pipeline* y por lo tanto podemos llamarlo más adelante en el pipeline.\n",
    "\n",
    "+ **Pipeline:** Es una secuencia de procesos/etapas generado por *transformers* y *estimators* para hacer un *workflow* de ML. Cada *transformer*/*estimator* es una etapa dentro de la secuencia del *pipeline*, cada paso se corre en el orden establecido y el *DataFrame* de entrada es transformado por cada paso, si el paso es un *transformer* entonces se le aplica el método `transform` y si el paso es un `estimator` se le aplica el método `fit`. \n",
    "\n",
    "\n",
    "![](./images/spark_estimator_transformer.png)\n",
    "<br>\n",
    "\n",
    "Por ejemplo: Si tuviéramos un texto al cuál quisieramos aplicarle un análisis de sentimiento, el *pipeline* podría consistir en los siguientes pasos: \n",
    "\n",
    "Suposiciones: \n",
    "+ Tenemos un corpus.\n",
    "+ Tenemos las palabras asociadas a un sentimiento.\n",
    "\n",
    "+ Separar cada documento en palabras.\n",
    "+ Convertir cada palabra de cada documento en un vector numérico.\n",
    "+ Utilizando el vector numérico y las etiquetas asociadas -del sentimiento- ocupar un modelo de clasificación \n",
    "\n",
    "![](./images/spark_pipelines.png)\n",
    "<br>\n",
    "* Fuente: [Spark ML Guide](https://spark.apache.org/docs/2.4.7/ml-guide.html)\n",
    "\n",
    "\n",
    "![](./images/pointer.png) En Spark, un pipeline **es** un *estimator* (al igual que en `sklearn`, por lo que puede hacer llamada al método `fit`, al hacer esto se genera un *PipelineModel* -que es un *transformer*-. Cuando querramos ocupar modelos entrenados para producción deberemos ocupar el *PipelineModel* generado en el momento de entranamiento al hacer una llamada a su método *transform*, de esta manera todos los *estimators* del *pipeline* original son convertidos a *transformer* asegurándonos de que en pruebas tendremos los mismos pasos/trasnformaciones ocupados para el entrenamiento del modelo. ╭(◔ ◡ ◔)/\n",
    "\n",
    "![](./images/spark_pipeline_model.png)\n",
    "<br>\n",
    "* Fuente: [Spark ML Guide](https://spark.apache.org/docs/2.4.7/api/python/pyspark.ml.html#module-pyspark.ml) \n",
    "\n",
    "Un *Pipeline* en Spark está representado como un DAG, el ejemplo anterior es un DAG lineal, pero no necesariamente deben ser lineales, basta con que cumplan las características de ser un DAG -grafo **acíclico** dirigido-. Es por esta razón que cada instanciación de un *transformer* o *estimator* tiene asociado un ID y debe ser único, si necesitáramos un mismo *transformer* en el *pipeline* requerimos de generar otro *transformer* -aunque tenga el mismo código- :( (ya sé! esto medio que le da en la ma al principio de *reuse* pero ... por el momento así se resuelve en Spark en pro de tener un *pipeline*), Spark revisa en tiempo de ejecución que no se rompa \"algo\" antes de correr el *pipeline* -*lazy*-.\n",
    "\n",
    "+ **Parameter:** API con la que se pueden compartir parámetros entre *Estimators* y *Transformers*. Ocupamos el objeto `Param` que es un parámetro nombrado con documentación auto contenida en un `ParamMap` -diccionario de parámetro, valor-.\n",
    "\n",
    "En Spark hay dos maneras de pasar parámetros a los algoritmos de ML:\n",
    "\n",
    "1. Configurar los parámetros fijos de los algoritmos a ocupar (*setters*)\n",
    "2. Pasar un `ParamMap` con los parámetros y sus valores a través de `fit` o `transform`, si se envían parámetros de esta manera se hace *override* a los específicados vía *setters*\n",
    "\n",
    "Lo lindo de estos objetos es que cada definición dentro del `ParamMap` es \"atado\" a un *estimator* o *transformer* en específico -a través del ID antes mencionado-. Por ejemplo: si tuvieramos en un *pipeline* dos regresiones logísticas -`lr1` y `lr2`- podríamos ocupar un `ParamMap` que establezca el valor de las iteraciones máximas de cada regresión: \n",
    "\n",
    "#### I/O de *pipeline* o modelos\n",
    "\n",
    "Desde Spark 1.6 se agregó la posibilidad de guardar modelos y *pipelines* implementados en Spark para poder ocuparlos después, pero no todos los algoritmos de `spark.ml` tienen esta posibilidad -tiene que ver con la paridad que mencionamos al principio de la libería `spark.mllib` y la `spark.ml`-, por lo que se requiere revisar la documentación específica de cada algoritmo y ver si se puede y cómo. ¯\\\\_(ツ)_/¯\n",
    "\n",
    "El paquete `spark.ml.util` tiene los objetos `MLWriter` y `MLReader` que permiten guardar y cargar modelos sin importar el lenguaje en el que se hayan implementado -Scala, Java, Python o R (implementaciones **específicas** para Spark!)-\n",
    "\n",
    "**Pipeline:** [Spark ML API](https://spark.apache.org/docs/2.4.7/api/python/pyspark.ml.html#module-pyspark.ml)\n",
    "\n",
    "+ En Spark 2.4.7 existe el método `save(path)` para poder guardar un *pipeline* en el *path* indicado, es un *shortcut* a `write.save(path)` \n",
    "+ En Spark 2.4.7 existe el método `load(path)` para poder cargar un *pipeline*, es un *shortcut* a `read.load(path)`\n",
    "\n",
    "**Modelos:**\n",
    "\n",
    "Los modelos que tengan posibilidad de ser guardados tendrán los métodos de `save(path)` y `load(path)` (verificar documentación del API para el modelo ocupado)\n",
    "\n",
    "#### Ejemplos:\n",
    "\n",
    "1. Veremos un pequeño ejemplo de cómo ocupar los *estimators* y *transformers* (viene en la documentación de Spark).\n",
    "\n",
    "\n",
    "+ [Crear un DataFrame](https://spark.apache.org/docs/2.4.7/api/python/pyspark.sql.html#pyspark.sql.SparkSession)\n",
    "\n",
    "El código que se muestra en cada uno de los ejemplos no corre aquí directamente (necesita de spark!)\n",
    "\n",
    "El siguiente ejemplo se encuentra en la carpeta `scripts/spark/sparkml_logisit_regression.json` para que puedas cargarlo en Zeppelin y echarlo a andar. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pyspark \n",
    "from pyspark.ml.linalg import Vectors\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "\n",
    "# En Spark se puede crear un DataFrame de un RDD, de una lista o de un DataFrame de Pandas, \n",
    "# aquí lo estamos creando con una lista que contiene tuplas de (label, features)\n",
    "# tal cual lo hacíamos en sklearn, y le estamos agregando los nombres de cada columna.\n",
    "# Vectors.dense recibe una lista como parámetro\n",
    "# Este DataFrame lo estamos ocupando como nuestro set de entrenamiento mock!\n",
    "training = spark.createDataFrame([\n",
    "    (1.0, Vectors.dense([0.0, 1.1, 0.1])),\n",
    "    (0.0, Vectors.dense([2.0, 1.0, -1.0])),\n",
    "    (0.0, Vectors.dense([2.0, 1.3, 1.0])),\n",
    "    (1.0, Vectors.dense([0.0, 1.2, -0.5]))], [\"label\", \"features\"])\n",
    "\n",
    "# Como lo hacíamos en sklearn, primero configuramos los hiperparámetros del modelo que queremos\n",
    "# ocupar.\n",
    "lr = LogisticRegression(maxIter=10, regParam=0.01)\n",
    "# Veamos la documentación del modelo y qué parametros le pusimos a nuestra\n",
    "# configuración\n",
    "print(\"LogisticRegression parameters:\\n\" + lr.explainParams() + \"\\n\")\n",
    "\n",
    "# Ocupemos el modelo que configuramos para entrenar con lo datos que\n",
    "# creamos en el DataFrame training\n",
    "model_1 = lr.fit(training)\n",
    "\n",
    "# model_1 es un transfomer creado a traves de un estimador (LogisticRegression)\n",
    "print(\"Model 1 was fit using parameters: \")\n",
    "# aquí estamos obteniendo la configuración con la que se entrenó\n",
    "# la regresión logística que ocupamos\n",
    "print(lr.extractParamMap())\n",
    "\n",
    "\n",
    "# Tambien podemos especificar los parametros con los que queremos que \n",
    "# corra el modelo utilizando el diccionario de ParamMap\n",
    "# Creamos un diccionario -se puede llamar como quieras!- que tenga\n",
    "# como llave el nombre del parametro que quieres modificar, con el valor\n",
    "# correspondiente.\n",
    "param_map = {lr.maxIter: 20}\n",
    "# Si el valor ya existe en el diccionario puedes actualizarlo\n",
    "param_map[lr.maxIter] = 30  \n",
    "# Tambien puedes actualizar varios parametros del diccionario al mismo tiempo\n",
    "param_map.update({lr.regParam: 0.1, lr.threshold: 0.55}) \n",
    "\n",
    "# Se pueden combinar diferentes diccionarios...realmente puedes tener\n",
    "# un solo diccionario con los parámetros de diversos modelos que ocupes en el\n",
    "# pipeline sin ningun problema, pues el valor asociado es por objeto (ID)\n",
    "# Aquí estamos cambiando el nombre de la columna que guarda la salida del\n",
    "# modelo, por default se llama 'probability' -> verificar documentacion del \n",
    "# metodo\n",
    "param_map_2 = {lr.probabilityCol: \"my_probability\"}  \n",
    "param_map_combined = param_map.copy()\n",
    "param_map_combined.update(param_map_2)\n",
    "#puedes ver el contenido del diccionario con param_map_combined.items() -> python 3.5.2\n",
    "\n",
    "# Entrenemos una segunda regresión logística con los nuevos parámetros que \n",
    "# establecimos a traves del paramMap\n",
    "# En este fit estamos enviando tanto los datos como los parámetros a ocupar en el\n",
    "# modelo de regresión logística\n",
    "model_2 = lr.fit(training, param_map_combined)\n",
    "print(\"Model 2 was fit using parameters: \")\n",
    "# aqui queremos ver con qué parámetros se quedó configurado el modelo\n",
    "# que ocupamos para entrenar\n",
    "print(lr.extractParamMap())\n",
    "\n",
    "# Creemos el data frame que tendrá los datos de prueba mock!\n",
    "test = spark.createDataFrame([\n",
    "    (1.0, Vectors.dense([-1.0, 1.5, 1.3])),\n",
    "    (0.0, Vectors.dense([3.0, 2.0, -0.1])),\n",
    "    (1.0, Vectors.dense([0.0, 2.2, -1.5]))], [\"label\", \"features\"])\n",
    "\n",
    "# Ahora sí, hagamos predicciones sobre el *set* de pruebas utilizando \n",
    "# el método `transform` (aquí no hay `predict`!)\n",
    "prediction = model_2.transform(test)\n",
    "# la respuesta es un DataFrame (la salida de un transform en su DataFrame)\n",
    "# por lo que podemos aplicarle los metodos de SparkSQL :)\n",
    "# verificamos que si es un DataFrame...\n",
    "type(prediction)\n",
    "# veamos que columnas tiene este DataFrame (como el names de R)\n",
    "prediction.columns\n",
    "# Aqui estamos seleccionando las columnas features, label, \n",
    "# my_probability -> que es el nombre que nosotros especificamos anteriormente en\n",
    "# ParamMap, y la columna prediction que es el nombre por default que regresa\n",
    "# el modelo al parametro 'predictionCol' -> ver documentacion\n",
    "# el collect hara que se regresen los resultados al drive!!! \n",
    "result = prediction.select(\"features\", \"label\", \"my_probability\", \"prediction\") \\\n",
    "    .collect()\n",
    "\n",
    "\n",
    "for row in result:\n",
    "    print(\"features={}, label={} -> prob={}, prediction={}\".format( \\\n",
    "    row.features, row.label, row.my_probability, row.prediction))\n",
    "    \n",
    "type(model_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "¿Qué pasó en este ejemplo?\n",
    "\n",
    "![](./images/spark_ex1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Ahora hagamos el ejemplo de texto para ver cómo se hacen los *pipeline* en Spark (viene en la documentación de Spark).\n",
    "\n",
    "\n",
    "+ [Tokenizer](https://spark.apache.org/docs/latest/api/python/pyspark.ml.html#pyspark.ml.feature.Tokenizer)\n",
    "+ [HashingTF (Hashing with Term Frequency -> MurmurHash3)](https://spark.apache.org/docs/2.1.0/api/python/pyspark.ml.html#pyspark.ml.feature.HashingTF)\n",
    "\n",
    "El siguiente ejemplo se encuentra en la carpeta `scripts/spark/sparkml_pipeline.json` para que puedas cargarlo en Zeppelin y echarlo a andar. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pyspark\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml.feature import HashingTF, Tokenizer\n",
    "    \n",
    "# al igual que en el ejemplo anterior, creamos un dataframe a través\n",
    "# de una lista con los datos de entrenamiento, la lista esta formada\n",
    "# por tuplas (id, texto, label). Esta forma no nos servirá para poder meterla\n",
    "# en los objetos de ML, pero más adelante arreglaremos esto\n",
    "training = spark.createDataFrame([\n",
    "    (0, \"a b c d e spark\", 1.0),\n",
    "    (1, \"b d\", 0.0),\n",
    "    (2, \"spark f g h\", 1.0),\n",
    "    (3, \"hadoop mapreduce\", 0.0)\n",
    "], [\"id\", \"text\", \"label\"])\n",
    "\n",
    "\n",
    "# Definimos los transformers: Tokenizer y HashingTF, y los \n",
    "# estimators: LogisticRegression que ocuparemos. Nota que aquí no hemos hecho\n",
    "# ningun fit todavia... la magia vendrá más adelante ;)\n",
    "# Tokenizer convierte el string de entrada (inputCol) a minúsculas y separa en\n",
    "# palabras utilizando como separador el espacio\n",
    "tokenizer = Tokenizer(inputCol=\"text\", outputCol=\"words\")\n",
    "# HashingTF permite hashear cada palabra utilizando MurmurHash3 convirtiendo\n",
    "# el hash generado en el índice a poner en el \"TDM\". Este método optimiza el\n",
    "# tiempo para generar el TDM de TF-IDF \"normal\". Para evitar colisiones en\n",
    "# la conversión a hash se aumenta el número de buckets -se recomienda ocupar\n",
    "# potencias de 2 para balancear las cubetas-\n",
    "# Nota que en este transformer estamos ocupando como entrada la salida del\n",
    "# transformer Tokenizer\n",
    "hashingTF = HashingTF(inputCol=tokenizer.getOutputCol(), outputCol=\"features\")\n",
    "# Ocuparemos una regresión logistica de nuex\n",
    "lr = LogisticRegression(maxIter=10, regParam=0.001)\n",
    "# Aqui viene lo bonito... definimos un pipeline que tiene como etapas/pasos\n",
    "# primero el tokenizer, luego el hashing y luego la regresión logística. Aquí\n",
    "# estamos definiendo el flujo de procesamiento, el DAG! \n",
    "pipeline = Pipeline(stages=[tokenizer, hashingTF, lr])\n",
    "\n",
    "\n",
    "# Voila, solo se requiere de hacer fit al pipeline para que esto funcione\n",
    "# como un pipeline, siguiendo el orden de los pasos establecidos en la \n",
    "# definicion del pipeline :) ... recuerda que el fit hace \n",
    "# el entrenamiento una vez que ya definimos las configuraciones de \n",
    "# los objetos que ocuparemos (transformers y estimators)\n",
    "model = pipeline.fit(training)\n",
    "\n",
    "# Creamos el dataframe de pruebas mock! -> Nota que aqui no hay \n",
    "# label!!!! (asi funcionaría en producción cierto!)\n",
    "test = spark.createDataFrame([\n",
    "    (4, \"spark i j k\"),\n",
    "    (5, \"l m n\"),\n",
    "    (6, \"spark hadoop spark\"),\n",
    "    (7, \"apache hadoop\")\n",
    "], [\"id\", \"text\"])\n",
    "\n",
    "\n",
    "# Lixto, \"ejecutamos\" el pipeline haciendo un transform al pipeline para \n",
    "# obtener las predicciones del set de pruebas\n",
    "prediction = model.transform(test)\n",
    "\n",
    "# De nuevo, prediction es un DataFrame generado con un transformer generado\n",
    "# a través de estimadores y transformers :) \n",
    "# Seleccionamos las columnas que queremos ver \n",
    "selected = prediction.select(\"id\", \"text\", \"probability\", \"prediction\")\n",
    "for row in selected.collect():\n",
    "    rid, text, prob, prediction = row\n",
    "    print(\"({}, {}) --> prob={}, prediction={}\".format( \\\n",
    "    rid, text, str(prob), prediction))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](./images/spark_ex2.png)\n",
    "<br>\n",
    "\n",
    "#### Feature engineering\n",
    "\n",
    "En este módulo de la librería de ML (`pyspark.ml.feature`) se encuentran las funciones asociadas a las siguientes acciones: \n",
    "\n",
    "+ **Feature extraction:** Extraer características -*features*- de datos crudos -*raw*-\n",
    "+ **Feature transformation:** Escalar, convertir o modificar características -*features*-\n",
    "+ **Feature selection:** Seleccionar un subconjunto de variables/características -*feataures*- de un conjunto más amplio\n",
    "+ **Locality Sensitive Hashing (LSH):** Algoritmos que se ocupan para obtener *feature transformation* \n",
    "\n",
    "En el ejemplo del texto ocupamos métodos de *feature transformantion*: `Tokenizer`, y de LSH: `HashingTF`. En esta parte, veremos ejemplos de los métodos más utilizados en cada una de las categorías mencionadas, esta parte  no es en absoluto exhaustiva pues Spark cuenta con muchos métodos implementados, solo es para que se den una idea de cómo se ocupan en Spark. ([Spark ML feature API](https://spark.apache.org/docs/latest/ml-features.html))\n",
    "\n",
    "![](./images/spark_feature_module.png)\n",
    "<br>\n",
    "\\*Fuente: [Spark ML Guide](https://spark.apache.org/docs/latest/ml-guide.html)\n",
    "\n",
    "3. **Feature extraction**\n",
    "\n",
    "a. **TF-IDF**\n",
    "\n",
    "[Spark TF-IDF](https://spark.apache.org/docs/latest/ml-features.html#tf-idf)\n",
    "\n",
    "Solo para recordar, TF-IDF es un algoritmo de minería de texto ocupado normalmente en problemas de IR a través del cual, contando la frecuencia de aparición de una palabra en todo la colección de documentos y en la frecuencia dentro de cada documento, se establece la relevancia de un documento dado un query de búsqueda. \n",
    "\n",
    "$$tf\\_idf=tf \\cdot log_{10}\\frac{N}{df}$$ \n",
    "\n",
    "En este ejemplo, TF-IDF se ocupa como una transformación a una variable *raw* -las palabras- para ser ocupadas como un *feature* en otro algoritmo de aprendizaje de máquina.\n",
    "\n",
    "El siguiente ejemplo se encuentra en la carpeta `scripts/spark/sparkml_idf.json` para que puedas cargarlo en Zeppelin y echarlo a andar. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pyspark\n",
    "from pyspark.ml.feature import HashingTF, IDF, Tokenizer, CountVectorizer\n",
    "\n",
    "# Creamos nuestro set de entrada para formar la TDM\n",
    "sentence_data = spark.createDataFrame([\n",
    "    (0.0, \"Hi I heard about Spark\"),\n",
    "    (0.0, \"I wish Java could use case classes\"),\n",
    "    (1.0, \"Logistic regression models are neat\")\n",
    "], [\"label\", \"sentence\"])\n",
    "\n",
    "# Ocupamos el transformer Tokenizer para separar por palabras\n",
    "tokenizer = Tokenizer(inputCol=\"sentence\", outputCol=\"words\")\n",
    "# Aqui no hay train! porque no estamos entrenando nanda... estamos en un problema\n",
    "# de IR. Tokenizer no tiene un metodo fit -no hay entrenamiento-\n",
    "words_data = tokenizer.transform(sentence_data)\n",
    "\n",
    "# Ocupamos el estimador CountVectorizer para generar una matriz de \n",
    "# terminos y sus frecuencias \n",
    "count_vectorizer = CountVectorizer(inputCol=\"words\", outputCol=\"raw_features\")\n",
    "featurized_model = count_vectorizer.fit(words_data)\n",
    "featurized_data = featurized_model.transform(words_data)\n",
    "featurized_data.show(truncate=False)\n",
    "\n",
    "# Ocupamos IDF para obtener el IDF de la coleccion de documentos mock que \n",
    "# generamos. IDF si tiene un metodo fit a traves del cual le enviamos el set \n",
    "# de tokens al que queremos obtener el IDF\n",
    "idf = IDF(inputCol=\"raw_features\", outputCol=\"features\", minDocFreq=1)\n",
    "# Aqui obtenemos el modelo a ocupar (transformer) a ocupar \n",
    "idf_model = idf.fit(featurized_data)\n",
    "rescaled_data = idf_model.transform(featurized_data)\n",
    "\n",
    "rescaled_data.select(\"label\", \"features\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](./images/spark_tfidf.png)\n",
    "<br>\n",
    "\n",
    "\n",
    "4. **Feature transformation** \n",
    "\n",
    "a. **OneHotEncoder** \n",
    "\n",
    "Solo para recordar, *one hot encoding* transforma una variable categórica de $n$ categorías a $n$ variables binarias, normalmente ocupamos esta transformación para ocupar variables categóricas en algoritmos que solo ocupan representaciones numéricas -normalmente aquellos algoritmos que ocupand distancias-\n",
    "\n",
    "\n",
    "+ [Spark OneHotEncoder](https://spark.apache.org/docs/2.1.0/ml-features.html#onehotencoder)\n",
    "+ [Spark StringIndexer](https://spark.apache.org/docs/2.1.0/ml-features.html#stringindexer)\n",
    "\n",
    "El siguiente ejemplo se encuentra en la carpeta `scripts/sparkml_one_hot_encoder.json` para que puedas cargarlo en Zeppelin y echarlo a andar. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pyspark\n",
    "from pyspark.ml.feature import OneHotEncoder, StringIndexer\n",
    "\n",
    "# creamos nuestro set de datos de entrada categorico\n",
    "df = spark.createDataFrame([\n",
    "    (0, \"a\"),\n",
    "    (1, \"b\"),\n",
    "    (2, \"c\"),\n",
    "    (3, \"a\"),\n",
    "    (4, \"a\"),\n",
    "    (5, \"c\")\n",
    "], [\"id\", \"category\"])\n",
    "\n",
    "# Esta funcion agrega un id numerico a cada valor diferente de un valor categorico \n",
    "# es como establecer los niveles en R de una factor pero los niveles son numericos,\n",
    "# sus id. El indice se establece por orden de frecuencia (descendente), por lo que \n",
    "# el indice 0 corresponde a la variable que aparece con mas frecuencia\n",
    "string_indexer = StringIndexer(inputCol=\"category\", outputCol=\"categoryIndex\")\n",
    "model = string_indexer.fit(df)\n",
    "indexed = model.transform(df)\n",
    "indexed.show()\n",
    "\n",
    "# OneHotEncoder no tiene un fit ya que solo es un transformador\n",
    "encoder = OneHotEncoder(inputCol=\"categoryIndex\", outputCol=\"categoryVec\")\n",
    "encoded = encoder.transform(indexed)\n",
    "encoded.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](./images/spark_onehot.png)\n",
    "<br>\n",
    "\n",
    "b. **MinMaxScaler** \n",
    "\n",
    "Es la normalización de minería de datos $\\frac{x-min}{max-min}$\n",
    "\n",
    "El siguiente ejemplo se encuentra en la carpeta `scripts/sparkml_minmax_scaler.json` para que puedas cargarlo en Zeppelin y echarlo a andar. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pyspark\n",
    "from pyspark.ml.feature import MinMaxScaler\n",
    "from pyspark.ml.linalg import Vectors\n",
    "\n",
    "data_frame = spark.createDataFrame([\n",
    "    (0, Vectors.dense([1.0, 0.1, -1.0]),),\n",
    "    (1, Vectors.dense([2.0, 1.1, 1.0]),),\n",
    "    (2, Vectors.dense([3.0, 10.1, 3.0]),)\n",
    "], [\"id\", \"features\"])\n",
    "data_frame.show()\n",
    "\n",
    "# Configuramos el estimator MinMaxScaler como lo necesitamos\n",
    "scaler = MinMaxScaler(inputCol=\"features\", outputCol=\"scaled_features\")\n",
    "\n",
    "# Creamos el modelo MinMaxScaler (transformer)\n",
    "scaler_model = scaler.fit(data_frame)\n",
    "\n",
    "# Transformamos los datos reescalando \n",
    "scaled_data = scaler_model.transform(data_frame)\n",
    "# Nota que cuando pedimos getMin y getMax lo hacemos al estimator, no al modelo\n",
    "print(\"Features scaled to range: [{}, {}]\".format(scaler.getMin(), scaler.getMax()))\n",
    "scaled_data.select(\"features\", \"scaled_features\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](./images/spark_mimaxscaler.png)\n",
    "<br>\n",
    "\n",
    "c. **StandardScaler**\n",
    "\n",
    "Corresponde a la estandarización en minería de datos $\\frac{x-\\mu}{\\sigma}$\n",
    "\n",
    "El siguiente ejemplo se encuentra en la carpeta `scripts/sparkml_standard_scaler.json` para que puedas cargarlo en Zeppelin y echarlo a andar. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pyspark\n",
    "from pyspark.ml.linalg import Vectors\n",
    "from pyspark.ml.feature import StandardScaler\n",
    "\n",
    "# Creamos el data frame que queremos estandarizar\n",
    "data_frame = spark.createDataFrame([\n",
    "    (0, Vectors.dense([1.0, 0.1, -1.0]),),\n",
    "    (1, Vectors.dense([2.0, 1.1, 1.0]),),\n",
    "    (2, Vectors.dense([3.0, 10.1, 3.0]),)\n",
    "], [\"id\", \"features\"])\n",
    "# Configuramos el estimator StandarScaler como lo necesitamos (por default\n",
    "# withMean esta en False porque hace que se regrese un vector dense...\n",
    "# hay que tener cuidado con eso cuando estemos manejandoo vectores sparse\n",
    "scaler = StandardScaler(inputCol=\"features\", outputCol=\"scaled_features\",\n",
    "                        withStd=True, withMean=True)\n",
    "# Creamos el modelo StandardScaler para los datos de entrada\n",
    "scaler_model = scaler.fit(data_frame)\n",
    "\n",
    "# Transformamos los datos \n",
    "scaled_data = scaler_model.transform(data_frame)\n",
    "scaled_data.show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](./images/spark_standardscaler.png)\n",
    "<br>\n",
    "\n",
    "### Grid Search \n",
    "\n",
    "El procesamiento en paralelo ha habilitado la posibilidad de probar un algoritmo con diferentes configuraciones de hiperparámetros y también muchos posibles algoritmos al mismo problema para poder encontrar la mejor configuración asociada al problema y el algoritmo con mejor desempeño. \n",
    "\n",
    "El objeto `GridSearchCV` (estimador) de scklearn nos permite cubrir la primera posibilidad -probar diferentes configuraciones de hiperparámetros de un mismo algoritmo- ocupando el un diccionario `param_grid` en donde por cada parámetro a probar establecemos los diferentes valores que queremos explorar con la ventaja de poderlo probar con un *pipeline* asociado. \n",
    "\n",
    "Por otro lado, para cubrir la segunda posibilidad -probar con diferentes algoritmos- usualmente ocumapos un par de algoritmos diferentes de forma \"manual\" pasando de uno simple a otro más complejo. Sin embargo, cuando queremos probar diferentes algoritmos a un mismo problema para ver sus diferencias en desempeño ocupamos un **magic loop** en donde especificamos diferentes algoritmos y además aprovechamos para poner diferentes configuraciones de hiperparámetros a cada algoritmo. \n",
    "\n",
    "Por ejemplo: Si quisieramos probar con un Árbol, un RandomForest y una Regresión Logística ($\\leftarrow$ ¿qué queremos hacer?), probando con diferentes hiperparámetros para cada algoritmo podríamos generar el siguiente script de *magic loop*. \n",
    "\n",
    "El siguiente ejemplo se encuentra en la carpeta `scripts/sparkml_grid_search.json` para que puedas cargarlo en Zeppelin y echarlo a andar. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "from pyspark.ml.feature import HashingTF, Tokenizer\n",
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
    "\n",
    "# Prepare training documents, which are labeled.\n",
    "training = spark.createDataFrame([\n",
    "    (0, \"a b c d e spark\", 1.0),\n",
    "    (1, \"b d\", 0.0),\n",
    "    (2, \"spark f g h\", 1.0),\n",
    "    (3, \"hadoop mapreduce\", 0.0),\n",
    "    (4, \"b spark who\", 1.0),\n",
    "    (5, \"g d a y\", 0.0),\n",
    "    (6, \"spark fly\", 1.0),\n",
    "    (7, \"was mapreduce\", 0.0),\n",
    "    (8, \"e spark program\", 1.0),\n",
    "    (9, \"a e c l\", 0.0),\n",
    "    (10, \"spark compile\", 1.0),\n",
    "    (11, \"hadoop software\", 0.0)\n",
    "], [\"id\", \"text\", \"label\"])\n",
    "\n",
    "# Configure an ML pipeline, which consists of tree stages: tokenizer, hashingTF, and lr.\n",
    "tokenizer = Tokenizer(inputCol=\"text\", outputCol=\"words\")\n",
    "hashingTF = HashingTF(inputCol=tokenizer.getOutputCol(), outputCol=\"features\")\n",
    "lr = LogisticRegression(maxIter=10)\n",
    "pipeline = Pipeline(stages=[tokenizer, hashingTF, lr])\n",
    "\n",
    "# We now treat the Pipeline as an Estimator, wrapping it in a CrossValidator instance.\n",
    "# This will allow us to jointly choose parameters for all Pipeline stages.\n",
    "# A CrossValidator requires an Estimator, a set of Estimator ParamMaps, and an Evaluator.\n",
    "# We use a ParamGridBuilder to construct a grid of parameters to search over.\n",
    "# With 3 values for hashingTF.numFeatures and 2 values for lr.regParam,\n",
    "# this grid will have 3 x 2 = 6 parameter settings for CrossValidator to choose from.\n",
    "paramGrid = ParamGridBuilder() \\\n",
    "    .addGrid(hashingTF.numFeatures, [10, 100, 1000]) \\\n",
    "    .addGrid(lr.regParam, [0.1, 0.01]) \\\n",
    "    .build()\n",
    "\n",
    "crossval = CrossValidator(estimator=pipeline,\n",
    "                          estimatorParamMaps=paramGrid,\n",
    "                          evaluator=BinaryClassificationEvaluator(), #solo auc o areaunderPR:(\n",
    "                          numFolds=5)  # use 3+ folds in practice\n",
    "\n",
    "# Run cross-validation, and choose the best set of parameters.\n",
    "cvModel = crossval.fit(training)\n",
    "\n",
    "# Prepare test documents, which are unlabeled.\n",
    "test = spark.createDataFrame([\n",
    "    (4, \"spark i j k\"),\n",
    "    (5, \"l m n\"),\n",
    "    (6, \"mapreduce spark\"),\n",
    "    (7, \"apache hadoop\")\n",
    "], [\"id\", \"text\"])\n",
    "\n",
    "# Make predictions on test documents. cvModel uses the best model found (lrModel).\n",
    "prediction = cvModel.transform(test)\n",
    "selected = prediction.select(\"id\", \"text\", \"probability\", \"prediction\")\n",
    "for row in selected.collect():\n",
    "    print(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "itam",
   "language": "python",
   "name": "itam"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
