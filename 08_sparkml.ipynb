{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](./docs/images/itam_logo.png)\n",
    "\n",
    "M. Sc. Liliana Millán Núñez liliana.millan@itam.mx\n",
    "\n",
    "Marzo 2020\n",
    "\n",
    "## SparkML\n",
    "\n",
    "### Agenda \n",
    "\n",
    "+ SparkML\n",
    "    + Pipelines\n",
    "    + Feature engineering\n",
    "    + Clasificación y regresión\n",
    "    + Agrupación\n",
    "    + Tuneo de hiperparámetros\n",
    "+ Ejemplo\n",
    "\n",
    "### Spark ML\n",
    "\n",
    "`spark.ml` es el módulo de *machine learning* de Spark, diseñado para realizar *machine learning* dentro de spark de manera escalable, sencilla y aprovechando el procesamiento en paralelo.\n",
    "\n",
    "**Características** \n",
    "\n",
    "+ Tiene algoritmos de ML ya implementados con la modificaciones necesarias para aprovechar el ambiente distribuido donde vive Spark: clasificación, regresión, agrupación, filtros colaborativos, etc. \n",
    "+ Tiene implementaciones de funciones que ocupamos para hacer *feature engineering*: *feature extraction*, *feature selection*, transformaciones, reducción de dimensionalidad.\n",
    "+ Permite generar *pipelines* en Spark al estilo de los pipelines de `scikitlearn`.\n",
    "+ Tiene una libería de utilerías con álgebra lineal, estadística, manejo de datos, etc.\n",
    "\n",
    "¿Por qué hay un `spark.mllib` y un `spark.ml`? \n",
    "\n",
    "En la primer versión de Spark no existía la abstracción de *DataFrame* -el *wrapper* de los RDD- y todos los algoritmos de ML desarrollados en Spark interactuaban directamente con el RDD, todas estas implementaciones se encuentran en el paquete `spark.mllib` -que ya está descontinuada-. Una vez que salió la versión 2 de Spark y con ella los nuevos objetos *SparkSession* y *DataFrame* los algoritmos de ML fueron modificados -algunos- para que solo tengan interacción con la abstracción *DataFrame* y con ello surgió la librería `spark.ml` que es la que utilizaremos nosotros. Aún no están todos los algoritmos de `spark.mllib` implementados en *DataFrame*, en la versión 2.4.5 de Spark, la librería de `spark.mllib` ya está en estatus de solo mantenimiento para que a partir de Spark 3.0 la librería será removida completamente de Spark y solo ocupar la interacción con los *DataFrames*. \n",
    "\n",
    "*Anyway* Para confundir más a la banda, el nombre oficial de la herramienta que ocupa Spark para ML se conoce como **MLlib** (╯°□°)╯︵ ┻━┻ aunque realmente se refieren a la librería `spark.ml`.\n",
    "\n",
    "[Spark ML API](https://spark.apache.org/docs/latest/ml-guide.html)\n",
    "\n",
    "#### Pipelines \n",
    "\n",
    "El diseño de los *pipelines* de Spark está inspirado en los *pipelines* de `scikit-learn`. Un *pipeline* en Spark está formado por los siguientes elementos: \n",
    "\n",
    "+ **DataFrame:** API que ocupa los *DataFrame* de SparkSQL para poder agregar otros tipos de datos que pueden ser útiles para ML -*vector*-\n",
    "\n",
    "+ **Transformer:** Algoritmo que transforma un *DataFrame* en otro DataFrame, recuerda que los *DataFrame* en Spark envuelven a un RDD y un RDD no puede ser modificado!. Para hacer una transformación se ocupa el método `transform()`. Los casos en los que ocuparemos un `transform` pueden ser agregar una nueva columna -por ejemplo *feature engineering*-, o por ejemplo una vez que se ha pasado un modelo de aprendizaje poner la respuesta final del modelo como parte del *DataFrame* original -etiqueta, score-. \n",
    "\n",
    "+ **Estimator:** Algoritmos que se aplican a un *DataFrame* para producir un *Transformer*. Los estimators son los que ocupan el método `fit()` para poder realizar un entrenamiento. El método `fit` recibe como parámetro un *DataFrame* y devuelve un modelo -que es un *transformer*-. Por ejemplo: Un algoritmo de regresión lineal es un *estimator* que tiene su método `fit` a través del cual entrena el algoritmo. \n",
    "\n",
    "\n",
    "$\\rightarrow$ Es importante conocer que por cada instancia de un *transformer* o *estimator* se genera un ID a través del cuál es reconocido durante todo el *pipeline* y por lo tanto podemos llamarlo más adelante en el pipeline.\n",
    "\n",
    "+ **Pipeline:** Es una secuencia de procesos/etapas generado por *transformers* y *estimators* para hacer un *workflow* de ML. Cada *transformer*/*estimator* es una etapa dentro de la secuencia del *pipeline*, cada paso se corre en el orden establecido y el *DataFrame* de entrada es transformado por cada paso, si el paso es un *transformer* entonces se le aplica el método `transform` y si el paso es un `estimator` se le aplica el método `fit`. \n",
    "\n",
    "\n",
    "![](./docs/images/spark_estimator_transformer.png)\n",
    "<br>\n",
    "\n",
    "Por ejemplo: Si tuviéramos un texto al cuál quisieramos aplicarle un análisis de sentimiento, el *pipeline* podría consistir en los siguientes pasos: \n",
    "\n",
    "Suposiciones: \n",
    "+ Tenemos un corpus.\n",
    "+ Tenemos las palabras asociadas a un sentimiento.\n",
    "\n",
    "+ Separar cada documento en palabras.\n",
    "+ Convertir cada palabra de cada documento en un vector numérico.\n",
    "+ Utilizando el vector numérico y las etiquetas asociadas -del sentimiento- ocupar un modelo de clasificación \n",
    "\n",
    "![](./docs/images/spark_pipelines.png)\n",
    "\n",
    "<br>\n",
    "\\* Fuente: [Spark ML Guide](https://spark.apache.org/docs/latest/ml-guide.html)\n",
    "\n",
    "\n",
    "![](./docs/images/pointer.png) En Spark, un pipeline **es** un *estimator* (al igual que en `sklearn`, por lo que puede hacer llamada al método `fit`, al hacer esto se genera un *PipelineModel* -que es un *transformer*-. Cuando querramos ocupar modelos entrenados para producción deberemos ocupar el *PipelineModel* generado en el momento de entranamiento al hacer una llamada a su método *transform*, de esta manera todos los *estimators* del *pipeline* original son convertidos a *transformer* asegurándonos de que en pruebas tendremos los mismos pasos/trasnformaciones ocupados para el entrenamiento del modelo. ╭(◔ ◡ ◔)/\n",
    "\n",
    "![](./docs/images/spark_pipeline_model.png)\n",
    "<br>\n",
    "\\*Fuente: [Spark ML Guide](https://spark.apache.org/docs/latest/api/python/pyspark.ml.html#module-pyspark.ml) \n",
    "\n",
    "Un *Pipeline* en Spark está representado como un DAG, el ejemplo anterior es un DAG lineal, pero no necesariamente deben ser lineales, basta con que cumplan las características de ser un DAG -grafo **acíclico** dirigido-. Es por esta razón que cada instanciación de un *transformer* o *estimator* tiene asociado un ID y debe ser único, si necesitaramos un mismo *transformer* en el *pipeline* requerimos de generar otro *transformer* -aunque tenga el mismo código- :( (ya sé! esto medio que le da en la ma al principio de *reuse* pero ... por el momento así se resuelve en Spark en pro de tener un *pipeline*), Spark revisa en tiempo de ejecución que no se rompa \"algo\" antes de correr el *pipeline* -*lazy*-\n",
    "\n",
    "+ **Parameter:** API con la que se pueden compartir parámetros entre *Estimators* y *Transformers*. Ocupamos el objeto `Param` que es un parámetro nombrado con documentación auto contenida en un `ParamMap` -diccionario de parámetro, valor-.\n",
    "\n",
    "En Spark hay dos maneras de pasar parámetros a los algoritmos de ML:\n",
    "\n",
    "1. Configurar los parámetros fijos de los algoritmos a ocupar (*setters*)\n",
    "2. Pasar un `ParamMap` con los parámetros y sus valores a través de `fit` o `transform`, si se envían parámetros de esta manera se hace *override* a los específicados vía *setters*\n",
    "\n",
    "Lo lindo de estos objetos es que cada definición dentro del `ParamMap` es \"atado\" a un *estimator* o *transformer* en específico -a través del ID antes mencionado-. Por ejemplo: si tuvieramos en un *pipeline* dos regresiones logísticas -`lr1` y `lr2`- podríamos ocupar un `ParamMap` que establezca el valor de las iteraciones máximas de cada regresión: \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pyspark \n",
    "from pyspark.ml.linalg import Vectors\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "\n",
    "# En Spark se puede crear un DataFrame de un RDD, de una lista o de un DataFrame de Pandas, \n",
    "# aquí lo estamos creando con una lista que contiene tuplas de (label, features)\n",
    "# tal cual lo hacíamos en sklearn, y le estamos agregando los nombres de cada columna.\n",
    "# Vectors.dense recibe una lista como parámetro\n",
    "# Este DataFrame lo estamos ocupando como nuestro set de entrenamiento mock!\n",
    "training = spark.createDataFrame([\n",
    "    (1.0, Vectors.dense([0.0, 1.1, 0.1])),\n",
    "    (0.0, Vectors.dense([2.0, 1.0, -1.0])),\n",
    "    (0.0, Vectors.dense([2.0, 1.3, 1.0])),\n",
    "    (1.0, Vectors.dense([0.0, 1.2, -0.5]))], [\"label\", \"features\"])\n",
    "\n",
    "# Como lo hacíamos en sklearn, primero instanciamos el modelo que quremos\n",
    "# ocupar con los parámetros que nosotros queremos tener para este \n",
    "# modelo en particular --configuramos el modelo--\n",
    "lr = LogisticRegression(maxIter=10, regParam=0.01)\n",
    "# Veamos la documentacion del modelo y que parametros le pusimos a nuestra\n",
    "# configuracion\n",
    "print(\"LogisticRegression parameters:\\n\" + lr.explainParams() + \"\\n\")\n",
    "\n",
    "# Ocupemos el modelo que configuramos para entrenar con lo datos que\n",
    "# creamos en el DataFrame training\n",
    "model_1 = lr.fit(training)\n",
    "\n",
    "# model_1 es un transfomer creado a traves de un estimador (LogisticRegression)\n",
    "print(\"Model 1 was fit using parameters: \")\n",
    "# aqui estamos obteniendo la configuracion con la que se entreno\n",
    "# la regresion logistica que ocupamos\n",
    "print(lr.extractParamMap())\n",
    "\n",
    "\n",
    "# Tambien podemos especificar los parametros con los que queremos que \n",
    "# corra el modelo utilizando el diccionario de ParamMap\n",
    "# Creamos un diccionario -se puede llamar como quieras!- que tenga\n",
    "# como llave el nombre del parametro que quieres modificar, con el valor\n",
    "# correspondiente.\n",
    "param_map = {lr.maxIter: 20}\n",
    "# Si el valor ya existe en el diccionario puedes actualizarlo\n",
    "param_map[lr.maxIter] = 30  \n",
    "# Tambien puedes actualizar varios parametros del diccionario al mismo tiempo\n",
    "param_map.update({lr.regParam: 0.1, lr.threshold: 0.55}) \n",
    "\n",
    "# Se pueden combinar diferentes diccionarios...realmente puedes tener\n",
    "# un solo diccionario con los parametros de diversos modelos que ocupes en el\n",
    "# pipeline sin ningun problema, pues el valor asociado es por objeto (ID)\n",
    "# aqui estamos cambiando el nombre de la columna que guarda la salida del\n",
    "# modelo, por default se llama 'probability' -> verificar documentacion del \n",
    "# metodo\n",
    "param_map_2 = {lr.probabilityCol: \"my_probability\"}  \n",
    "param_map_combined = param_map.copy()\n",
    "param_map_combined.update(param_map_2)\n",
    "#puedes ver el contenido del diccionario con param_map_combined.items() -> python 3.5.2\n",
    "\n",
    "# Entrenemos una segunda regresion logistica con los nuevos parametros que \n",
    "# establecimos a traves del paramMap\n",
    "# En este fit estamos enviando tanto los datos como los parametros a ocupar en el\n",
    "# modelo de regresion logistica\n",
    "model_2 = lr.fit(training, param_map_combined)\n",
    "print(\"Model 2 was fit using parameters: \")\n",
    "# aqui queremos ver cono que parametros se quedo configurado el modelo\n",
    "# que ocupamos para entrenar\n",
    "print(lr.extractParamMap())\n",
    "\n",
    "# Creemos el data frame que tendra los datos de prueba mock!\n",
    "test = spark.createDataFrame([\n",
    "    (1.0, Vectors.dense([-1.0, 1.5, 1.3])),\n",
    "    (0.0, Vectors.dense([3.0, 2.0, -0.1])),\n",
    "    (1.0, Vectors.dense([0.0, 2.2, -1.5]))], [\"label\", \"features\"])\n",
    "\n",
    "# Ahora sí, hagamos predicciones sobre el *set* de pruebas utilizando \n",
    "# el método `transform` (aquí no hay `predict`!)\n",
    "prediction = model_2.transform(test)\n",
    "# la respuesta es un DataFrame (la salida de un transform en su DataFrame)\n",
    "# por lo que podemos aplicarle los metodos de SparkSQL :)\n",
    "# verificamos que si es un DataFrame...\n",
    "type(prediction)\n",
    "# veamos que columnas tiene este DataFrame (como el names de R)\n",
    "prediction.columns\n",
    "# Aqui estamos seleccionando las columnas features, label, \n",
    "# my_probability -> que es el nombre que nosotros especificamos anteriormente en\n",
    "# ParamMap, y la columna prediction que es el nombre por default que regresa\n",
    "# el modelo al parametro 'predictionCol' -> ver documentacion\n",
    "# el collect hara que se regresen los resultados al drive!!! \n",
    "result = prediction.select(\"features\", \"label\", \"my_probability\", \"prediction\") \\\n",
    "    .collect()\n",
    "\n",
    "\n",
    "for row in result:\n",
    "    print(\"features={}, label={} -> prob={}, prediction={}\".format( \\\n",
    "    row.features, row.label, row.my_probability, row.prediction))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "itam",
   "language": "python",
   "name": "itam"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
